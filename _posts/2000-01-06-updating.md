---
title: "Set of movies"
bg: blue_5
color: white
fa-icon: cloud-upload
---

# Which set of movies do we have to send to the aliens to correctly represent Earth?

**Résumé opérationnel**

From now on, we leave the point of view of aliens, and adopt the one of scientists on Earth who aim at elaborating the optimal movie dataset to send to space, so as to represent the Earth accurately. How to find the best pool? And how many movies shoud this pool contain?
To answer these questions, we begin with defining a metric that assesses scores of pools of movies. We then optimize the number *N_opt* that should be sent. We can now iteratively construct our optimal pool. This enables us to analyze the sensitivity of our scoring method. We also use machine learning methods such as linear regressions and random forests to assess the influences of movies genres on the scores.

* Definition of the metric

We first define a metric that evaluates the score of pools of movies. This score of a pool is subdivided into 4 distinct subscores, that evaluate the fit of the pool to references in the USA: a score on parity (men to women ratio); a score on distribution of heights; a score on distribution of ages; and a score on diversity of ethnicities.
Description of each metric:
1.	For the parity score, we want to emphasize the fact that pools of movies that have a women (or men) ratio under 1/3 or over 2/3 should be strongly penalized. Indeed, parity an critical characteristic of human society. If that proportion gets close to 50%, the score rises rapidly to values close to 1. Also, the score should be symmetric in men and women, i.e. a pool with a proportion *p* of females should get the same score as a pool with proportion *1-p*.
2.	For the diversity score, we wish to attribute great scores to pools of movies that collectively represent as many ethnicities as possible. If the number of different ethnicities, *N_eth*, is at least 0.75* the number of actors, *N_act*, a score of 1 is attributed. The score then decreases with this ratio. This way, we value all ethnicities the same way, and value a form of diversity that is easy to measure.
3. For the height score, we observe the reference distribution for the United States, and compare our distribution over the actors to the reference one. We choose to penalize equally all misfits. We are likely to observe pools of movies that underrepresent either old or young people. Consequently, a pool that would not underrepresent them too much are way more accurate as regards total population, and deserve good scores.
4. For the age score, we apply the same strategy. Similarly, we choose to penalize equally all misfits. The same conclusions can be drawn.


Insert scatterplot Matrix

* Why do we assess the scores of pools of movies and not of movies?

A movie alone may have not enough actors to be representative of society. It also may be about a particular event, place, era, that does not describe comprehensively our society (wars under the Roman Empire). It may even be about science-fiction. For these reasons, we have concluded that scores over one movie are likely to be very insignificant. 
This traduces into a fairly high variance when we look at the distribution of scores over pools of one movie only. 

Insert plots about movies alone, it’s shit

Then, is there an optimal number of movies per pool, 
*N_opt*?

The following tradeoff occurs: if N increases, the variance of the scores of K pools of N movies should decrease. But on average, the film industry is not representative of society, with obvious biases in gender and ethnicities. Therefore, an increase in N should result in increases in the bias.

Insert plot of characteristics defined as a function of N


We choose N = 20 movies per pool as of now. We can also notice that the average number of actors per movie is around 8. This means that for 20 movies, we will do statistics over about 160 actors. This seems sufficient to be somehow representative.


* How to find the optimal pool of 20 movies?

The strategy here is not to find the $\binom{N_{movies}}{20}$. This would be disproportionately complex. We rather apply the following idea. First, we create 10,000 random pools of 10 films, and keep the best 10. From this basis, we iterate over all remaining movies to find the one that most improves the total score. We then have 11 movies, and redo the exact same thing, until the pool is composed of 20 movies.

* Is our model sensitive to changes in the scoring function?

A very arbitrary parameter we chose is the list of weightings that we attribute to each subscore, to sum up to the total score. What happens if this weighting is modified? Does an optimal pool for a previous scoring remain good? For this purpose, we define 6 different weightings, and evaluate 


2.	One idea to evaluate the influence of genres of movies is to do the following types of testes :Drop all drama, see average of scoring: does it change?  p values, t test
3.	One other idea to do that: Perform analyses on the impact of genres & box-offices of movies on total score, by random forests or linear regression : with features + genres, VS only features are conclusions significant? expectable ? + how to do them  Results ; what deliverable??? – t test
4.	Perform analyses on the sensitivity of our weighting, by modifying it and observing if good movies remain, or rather the score of other pools of movies  a la fin, is our method sensitive to a change in weighting? How will eventual aliens choose their own weighting? Livrable = graphe 2D, colormap discrete k*k?  Method quality assessment


Point 3. Linear regression and random forests methods applied on X1 (with the following features), and X2 (same without genres of movies), show the following results. In both cases, the prediction is quite accurate with respect to the test set (standard deviation respectively equal to.. and … for scores from 0 to 100). Now, is there any improvement enabled by the addition of the genres of the movies? To answer this question, we perform a T-test. Are results predicted by the model (random forests or linear regression) significantly better with the genres than without? It turns out that the difference is not significant, with p values of 0.54 and … . This confirms the slight gap between mean square errors of the models to the test set. The conclusion of this analysis is that knowing the main genres of movies in a pool do not hep predict its score.


Point 4. Weightings proposed: [3, 1.5, 1.5, 3] ; [1.5, 1.5, 1.5, 1.5] ; [3, 1.5, 1.5, 1.5] ; [1.5, 1.5, 1.5, 3] ; [1.5, 3, 1.5, 1.5] ; [1.5, 1.5, 3, 1.5]. 

We keep track of the pools that were optimal for one of these weightings, and we assess their scores for each of the other weightings. This way, we evaluate the sensitivity of our metric to variations in the way we compute the total score. 
Result. ?


