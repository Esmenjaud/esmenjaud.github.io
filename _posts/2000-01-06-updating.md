---
title: "Set of movies"
bg: bg5
color: white
fa-icon: cloud-upload
---

# Which set of movies do we have to send to the aliens to correctly represent Earth?

**Executive summary**

From now on, we leave the point of view of aliens, and adopt the one of scientists on Earth who aim at elaborating the optimal movie dataset to send to space, so as to represent the Earth accurately. How to find the best pool? And how many movies shoud this pool contain?
To answer these questions, we begin with defining a metric that assesses scores of pools of movies. We then optimize the number *N_opt* that should be sent. We can now iteratively construct our optimal pool. This enables us to analyze the sensitivity of our scoring method. We also use machine learning methods such as linear regressions and random forests to assess the influences of movies genres on the scores.

* Definition of the metric

We first define a metric that evaluates the score of pools of movies. This score of a pool is subdivided into 4 distinct subscores, that evaluate the fit of the pool to references in the USA: a score on parity (men to women ratio); a score on distribution of heights; a score on distribution of ages; and a score on diversity of ethnicities.
Description of each metric:
1.	For the parity score, we want to emphasize the fact that pools of movies that have a women (or men) ratio under 1/3 or over 2/3 should be strongly penalized. Indeed, parity an critical characteristic of human society. If that proportion gets close to 50%, the score rises rapidly to values close to 1. Also, the score should be symmetric in men and women, i.e. a pool with a proportion *p* of females should get the same score as a pool with proportion *1-p*.
2.	For the diversity score, we wish to attribute great scores to pools of movies that collectively represent as many ethnicities as possible. If the number of different ethnicities, *N_eth*, is at least 0.75* the number of actors, *N_act*, a score of 1 is attributed. The score then decreases with this ratio. This way, we value all ethnicities the same way, and value a form of diversity that is easy to measure.
3. For the height score, we observe the reference distribution for the United States, and compare our distribution over the actors to the reference one. We choose to penalize equally all misfits. We are likely to observe pools of movies that underrepresent either old or young people. Consequently, a pool that would not underrepresent them too much are way more accurate as regards total population, and deserve good scores.
4. For the age score, we apply the same strategy. Similarly, we choose to penalize equally all misfits. The same conclusions can be drawn.


Insert scatterplot Matrix

* Why do we assess the scores of pools of movies and not of movies?

A movie alone may have not enough actors to be representative of society. It also may be about a particular event, place, era, that does not describe comprehensively our society (wars under the Roman Empire). It may even be about science-fiction. For these reasons, we have concluded that scores over one movie are likely to be very insignificant. 
This traduces into a fairly high variance when we look at the distribution of scores over pools of one movie only. 

Insert plots about movies alone, it’s shit

Then, is there an optimal number of movies per pool, 
*N_opt*?

The following tradeoff occurs: if N increases, the variance of the scores of K pools of N movies should decrease. But on average, the film industry is not representative of society, with obvious biases in gender and ethnicities. Therefore, an increase in N should result in increases in the bias.

Insert plot of characteristics defined as a function of N


We choose N = 20 movies per pool as of now. We can also notice that the average number of actors per movie is around 8. This means that for 20 movies, we will do statistics over about 160 actors. This seems sufficient to be somehow representative.


* How to find the optimal pool of 20 movies?

The strategy here is not to find the $\binom{N_{movies}}{20}$. This would be disproportionately complex. We rather apply the following idea. First, we create 10,000 random pools of 10 films, and keep the best 10. From this basis, we iterate over all remaining movies to find the one that most improves the total score. We then have 11 movies, and redo the exact same thing, until the pool is composed of 20 movies.

* Is our model sensitive to changes in the scoring function?

A very arbitrary parameter we chose is the list of weightings that we attribute to each subscore, to sum up to the total score. What happens if this weighting is modified? Does an optimal pool for a previous scoring remain good? For this purpose, we define 6 different weightings, and evaluate the scores of these 10 pools on each new weighting. 

**Results**:

We observe that


we first take 100000 pools of movies, or 50000 pools of 20 movies?-. For each pool of movies, we have lots of features (such as the parity, ages and heights distributions of the actors in the 20 movies. This enables us to incorporate features such as average and std for height and age, and proportion of females.

The features we want to assess the influence are the genres of movies. About 300 genres, we choose to study the 10 most represented ones. In a pool of movies, we produce 10 features, each one corresponding to the frequency of the genre compared to the number of films in the pools (20).
    
We choose only the genres that are represented in 10% of movies at least, so that it should be quite representative.
    
    
We have a vector X of all rows and all features. y contains all the scores for all rows. 
 We separate them into a train e-and test sets (80-20). We use different models: linear regression and random forests to determine whether the genres have an influence. The graphs show the results for random forests for all features (left) and for features without genres.
    
The results are the following: very short difference in the std of the distributions of predicted scores. T test gives a small difference in their average values. 
and large p value --> difference is insignificant. So for now, we conclude that the genres we have studied do not influence the accuracy in predicting scores.


Point 3. Linear regression and random forests methods applied on X1 (with the following features), and X2 (same without genres of movies), show the following results. In both cases, the prediction is quite accurate with respect to the test set (standard deviation respectively equal to.. and … for scores from 0 to 100). Now, is there any improvement enabled by the addition of the genres of the movies? To answer this question, we perform a T-test. Are results predicted by the model (random forests or linear regression) significantly better with the genres than without? It turns out that the difference is not significant, with p values of 0.54 and … . This confirms the slight gap between mean square errors of the models to the test set. The conclusion of this analysis is that knowing the main genres of movies in a pool do not hep predict its score.


Point 4. Weightings proposed: [3, 1.5, 1.5, 3] ; [1.5, 1.5, 1.5, 1.5] ; [3, 1.5, 1.5, 1.5] ; [1.5, 1.5, 1.5, 3] ; [1.5, 3, 1.5, 1.5] ; [1.5, 1.5, 3, 1.5]. 

We keep track of the pools that were optimal for one of these weightings, and we assess their scores for each of the other weightings. This way, we evaluate the sensitivity of our metric to variations in the way we compute the total score. 
Result. ?


