---
title: "Set of movies"
bg: purple
color: white
fa-icon: cloud-upload
---

# Which set of movies do we have to send to the aliens to correctly represent Earth?

** Résumé opérationnel**

From now on, we leave the point of view of aliens, and adopt the one of scientists on Earth who aim at elaborating the optimal movie dataset to send to space, so as to represent the Earth accurately. How to find the best pool? And how many movies shoud this pool contain?
To answer these questions, we begin with defining a metric that assesses scores of pools of movies. We then optimize the number *N_opt* that should be sent. We can now iteratively construct our optimal pool. This enables us to analyze the sensitivity of our scoring method. We also use machine learning methods such as linear regressions and random forests to assess the influences of movies genres on the scores.

* Definition of the metric

We first define a metric that evaluates the score of pools of movies. This score of a pool is subdivided into 4 distinct subscores, that evaluate the fit of the pool to references in the USA: a score on parity (men to women ratio); a score on distribution of heights; a score on distribution of ages; and a score on diversity of ethnicities.
Description of each metric:
1.	For the parity score, we want to emphasize the fact that pools of movies that have a women (or men) ratio under 1/3 or over 2/3 should be strongly penalized. Indeed, parity an critical characteristic of human society. If that proportion gets close to 50%, the score rises rapidly to values close to 1. Also, the score should be symmetric in men and women, i.e. a pool with a proportion *p* of females should get the same score as a pool with proportion *1-p*.
2.	For the height score, we observe the reference distribution for the United States, and compare our distribution over the actors to the reference one. We choose to not penalize equally all misfits. We attribute high penalties to distributions that underrepresent either old or young people. This should happen in most pools of movies, but some pools that would not underrepresent them too much are way more accurate as regards total population, and deserve good scores.
3.	For the age score, we observe the reference distribution for the United States, and compare our distribution over the actors to the reference one. We choose to not penalize equally all misfits. We attribute high penalties to distributions that underrepresent either old or young people. This should happen in most pools of movies, but some pools that would not underrepresent them too much are way more accurate as regards total population, and deserve good scores.
4.	gg
Insert scatterplot Matrix
Note that we evaluate pools of movies rather than movies. Indeed, a movie alone may have not enough actors to be representative of society (ou il concerne un zome et laps de tremps trop court ?..... Scores over one movie are likely to be very insignificant. 
Insert plots about movies alone, it’s shit
Remains the question of N, the number of movies in a pool. The following tradeoff occurs: if N increases, we decrease variance. But on average, the film industry is not representative of society, with obvious biases in gender and ethnicities. Therefore, bias increases.
Insert plot of characteristics defined as a function of N
We choose N = 20 movies per pool as of now. We can also notice that the average number of actors per movie is around 8. This means that for 20 movies, we will do statistics over about 160 actors. This seems sufficient to be representative.

1.	Try to optimize for best pool of movies, any method  select a top…  Results ; deliverable???
2.	One idea to evaluate the influence of genres of movies is to do the following types of testes :Drop all drama, see average of scoring: does it change?  p values, t test
3.	One other idea to do that: Perform analyses on the impact of genres & box-offices of movies on total score, by random forests or linear regression : with features + genres, VS only features are conclusions significant? expectable ? + how to do them  Results ; what deliverable??? – t test
4.	Perform analyses on the sensitivity of our weighting, by modifying it and observing if good movies remain, or rather the score of other pools of movies  a la fin, is our method sensitive to a change in weighting? How will eventual aliens choose their own weighting? Livrable = graphe 2D, colormap discrete k*k?  Method quality assessment


Point 3. Linear regression and random forests methods applied on X1 (with the following features), and X2 (same without genres of movies), show the following results. In both cases, the prediction is quite accurate with respect to the test set (standard deviation respectively equal to.. and … for scores from 0 to 100). Now, is there any improvement enabled by the addition of the genres of the movies? To answer this question, we perform a T-test. Are results predicted by the model (random forests or linear regression) significantly better with the genres than without? It turns out that the difference is not significant, with p values of 0.54 and … . This confirms the slight gap between mean square errors of the models to the test set. The conclusion of this analysis is that knowing the main genres of movies in a pool do not hep predict its score.


Point 4. Weightings proposed: [3, 1.5, 1.5, 3] ; [1.5, 1.5, 1.5, 1.5] ; [3, 1.5, 1.5, 1.5] ; [1.5, 1.5, 1.5, 3] ; [1.5, 3, 1.5, 1.5] ; [1.5, 1.5, 3, 1.5]. 

We keep track of the pools that were optimal for one of these weightings, and we assess their scores for each of the other weightings. This way, we evaluate the sensitivity of our metric to variations in the way we compute the total score. 
Result. ?


